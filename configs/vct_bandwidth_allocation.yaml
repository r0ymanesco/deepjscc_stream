loss: 'l2'
train_epochs: 600
save_dir: 'checkpoints/vct_bw_allocation'
coding_stage: 200
predictor_stage: 400

dataset:
  dataset: 'ucf101'
  path: '/home/tt2114/datasets/UCF101'
  frames_per_clip: 25
  train_batch_size: 4
  eval_batch_size: 4

optimizer:
  solver: 'adam'
  lr: 0.0001
  fine_tune_loss_lmda: 1.0
  loss_modulator_lmda: 0.5

scheduler:
  scheduler: 'mult_lr'
  lr_schedule_factor: 0.8

encoder:
  target_quality: 25.0 # db psnr
  n_conditional_frames: 2
  c_in: 3
  c_feat: 192
  c_out: 128  # NOTE this can be any value (not bound to bw ratio)
  tf_layers: [6, 4, 5]
  tf_heads: 16
  tf_ff: 2048
  tf_dropout: 0.0
  c_win: 4
  p_win: 8

modem:
  modem: 'continuous'

channel:
  model: 'awgn'
  train_snr: [20.0]
  eval_snr: [20.0]

early_stop:
  mode: 'min'
  delta: 0.0
  patience: 10
